---
title: "Lightricks datahack challenge"
author: "Iyar Lin"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  github_document:
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F, cache = T)
set.seed(1)
options(scipen = 999)

packages <- c(
  "tidyverse", # best thing that happend to me
  "mlr", # ML framework
  "MLmetrics",
  "lubridate", # day of week 
  "pander", # pretty table rendering
  "xgboost", 
  "ranger", 
  "glmnet", 
  "parallelMap" # parallelStart function
)

sapply(
  packages,
  function(x) if (!require(x, character.only = TRUE, quietly = T)) {
      install.packages(x, quiet = T, verbose = F)
      library(x, character.only = T, quietly = T, verbose = F)
    }
)
```

# General

In this challenge I'll be utilizing as much as possible from the r package "mlr" (except for data preparation, as there's no missing data). I'll be committing every run with the F1 score.

# Load data and baseline features engineering

Should look at option to use days beyond 28.

```{r be sure to label chunks in an informative way}
train_users <- read_csv("train_users_data.csv") %>% 
  select(-c(X1, days_until_churn)) %>% 
  mutate(id = as.character(id))

train_usage <- read_csv("train_usage_data.csv") %>% 
  mutate(id = as.character(id))
test_users <- read_csv("test_users_data.csv") %>% 
  select(-X1) %>% 
  mutate(id = as.character(id))

test_usage <- read_csv("test_usage_data.csv") %>% 
  mutate(id = as.character(id))

users <- train_users %>% 
  mutate(dataset = "train") %>% 
  bind_rows(test_users %>% mutate(dataset = "test"))

usage <- train_usage %>% 
  mutate(dataset = "train") %>% 
  bind_rows(test_usage %>% mutate(dataset = "test"))
```

## Subscription local hour

I'm using a dataset from https://www.countries-ofthe-world.com/world-time-zones.html to find local time in each country.

```{r find local date and time}
UTC_offset <- read_csv("UTC_offset.csv")

users <- users %>% 
  mutate(country = replace(country, grepl("land Islands", country), "Aland Islands"), 
         country = replace(country, is.na(country), "United States")) %>% # most common country
  left_join(UTC_offset, by = "country") %>% 
  mutate(local_subscription_datetime = subscripiton_date + offset * 60 * 60, 
         local_subscription_hour = factor(as.numeric(format(local_subscription_datetime, "%H"))))

users %>% filter(dataset == "train") %>% 
  group_by(local_subscription_hour) %>% summarise(churned = mean(churned)) %>% 
  ggplot(aes(local_subscription_hour, churned)) + geom_col() + scale_y_continuous(labels = scales::percent)
```

```{r time of day distribution}
users %>% filter(dataset == "train") %>% 
  group_by(local_subscription_hour) %>% summarise(n = n()) %>% 
  ggplot(aes(local_subscription_hour, n)) + geom_col()
```

## Subscription day of week

```{r day of week}
users <- users %>% 
  mutate(subscripiton_day_of_week = factor(wday(local_subscription_datetime, label = T), ordered = F))
  
users %>% filter(dataset == "train") %>% 
  group_by(subscripiton_day_of_week) %>% summarise(churned = mean(churned)) %>% 
  ggplot(aes(subscripiton_day_of_week, churned)) + geom_col() + scale_y_continuous(labels = scales::percent)
```

```{r subscription day distribution}
users %>% filter(dataset == "train") %>% 
  group_by(subscripiton_day_of_week) %>% summarise(n = n()) %>% 
  ggplot(aes(subscripiton_day_of_week, n)) + geom_col()
```

## Country grouping

```{r country, results = "asis"}
users <- users %>% 
  mutate(country = factor(case_when(
           country %in% c("Sweden", "Netherlands", "Norway", "Denmark", "Ireland", "New Zealand", "Finland", "Germany", "Israel", "Belgium", "Switzerland", "France", "Spain", "Austria", "Italy", "Japan", "Iceland", "Andorra", "Portugal", "Monaco", "Malta", "Guernsey") ~ "Other developed",
           country %in% c("Romania", "Russia", "Poland", "Czechia", "Greece", "Ukraine", "Lithuania", "Hungary", "Slovakia", "Latvia", "Cyprus", "Bulgaria", "Moldova", "Kosovo", "Estonia", "Croatia", "Albania") ~ "Eastern Europe", 
           country %in% c("United States", "United Kingdom", "Australia", "Canada") ~ country,
           TRUE ~ "Other"
         ))
         )

pandoc.table(users %>% filter(dataset == "train") %>% group_by(country) %>% summarise(subscribers = n()))
```

```{r country churn}
users %>% filter(dataset == "train") %>% 
  group_by(country) %>% summarise(churned = mean(churned)) %>% 
  mutate(country = factor(country, levels = country[order(churned, decreasing = T)])) %>% 
  ggplot(aes(country, churned)) + geom_col() + scale_y_continuous(labels = scales::percent)
```

## App and ios versions

```{r}
users <- users %>% 
  mutate(initial_app_version = factor(substr(initial_app_version, 1, 3)), 
         initial_ios_version = factor(gsub(pattern = "\\..+", "", initial_ios_version)))

users %>% filter(dataset == "train") %>% 
  group_by(initial_app_version) %>% summarise(churned = mean(churned)) %>% 
  mutate(initial_app_version = factor(initial_app_version, levels = initial_app_version[order(churned, decreasing = T)])) %>% ggplot(aes(initial_app_version, churned)) + geom_col() + scale_y_continuous(labels = scales::percent)
```

```{r}
users %>% filter(dataset == "train") %>% 
  group_by(initial_ios_version) %>% summarise(churned = mean(churned)) %>% 
  mutate(initial_ios_version = factor(initial_ios_version, levels = initial_ios_version[order(churned, decreasing = T)])) %>% ggplot(aes(initial_ios_version, churned)) + geom_col() + scale_y_continuous(labels = scales::percent)
```

```{r device, results = "asis"}
pandoc.table(users %>% 
               filter(dataset == "train") %>% 
     group_by(initial_device) %>% 
     summarise(churned = mean(churned), n = n()) %>% 
 arrange(churned))
```

Some of these are pretty small groups so we'll do some grouping:

```{r}
users <- users %>% 
  mutate(initial_device = factor(case_when(
    initial_device %in% c("UIDeviceKindIPadMini3G", "UIDeviceKindIPod6G", "UIDeviceKindUnknownIPad", 
                          "UIDeviceKindUnknownIPhone") ~ "Other_low",
    initial_device %in% c("UIDeviceKindIPadAir2G", "UIDeviceKindIPadMini4G", "UIDeviceKindIPhone5S") ~ "Other_med", 
    initial_device %in% c("UIDeviceKindIPadAir1G", "UIDeviceKindIPadMini2G", "UIDeviceKindIPadPro2G12_9", 
                          "UIDeviceKindIPadPro9_7", "UIDeviceKindIPadPro10_5", "UIDeviceKindIPad5G", "UIDeviceKindIPadPro12_9") ~ "Other_high", 
    TRUE ~ initial_device
  )))

users %>% filter(dataset == "train") %>% 
  group_by(initial_device) %>% summarise(churned = mean(churned)) %>% 
  mutate(initial_device = factor(initial_device, levels = initial_device[order(churned, decreasing = T)])) %>% ggplot(aes(initial_device, churned)) + geom_col() + scale_y_continuous(labels = scales::percent) + theme(axis.text.x = element_text(angle = 90))
```


## Installation date vs subscription date

We can see below that subscribers were chosen based on subscription date, with installation date potentially very far in the past:

```{r}
users <- users %>% 
  mutate(installation_date = as.Date(installation_date))

users %>% filter(dataset == "train") %>% 
  ggplot(aes(installation_date)) + geom_histogram(binwidth = 1)
```

We'll bucket the difference between installation and subscription times.

```{r}
users <- users %>% 
  mutate(subscripiton_date = as.Date(subscripiton_date), 
         days_until_subscription = as.numeric(subscripiton_date - installation_date), 
         days_until_subscription_fac = cut(days_until_subscription, unique(quantile(days_until_subscription, seq(0, 1, .1))), include.lowest = T))

users %>% filter(dataset == "train") %>% 
  group_by(days_until_subscription_fac) %>% summarise(churned = mean(churned)) %>% 
  ggplot(aes(days_until_subscription_fac, churned)) + geom_col()
```

Baseline features summary:

```{r baseline features summary, results = "asis"}
users <- users %>% 
  select(id, country, initial_app_version, initial_ios_version, initial_device, 
         churned, local_subscription_hour, subscripiton_day_of_week, days_until_subscription_fac, dataset)

data.frame(variable = names(users),
           class = unname(sapply(users, class)),
           first_values = sapply(users, function(x) paste0(head(x, 4),  collapse = ", ")),
           row.names = NULL) %>% 
  pandoc.table(caption = "Baseline features dataset")
```


# usage features engineering

## Usage duration

```{r usage duration, results = "asis"}
pandoc.table(summary(usage$usage_duration[usage$dataset == "train"]), caption = "Usage duration distribution")
```

Let's have a closer look at those outliers:

```{r usage duration outliers, results="asis"}
usage %>% 
  filter(dataset == "train") %>% 
  summarise(p95 = quantile(usage_duration, .95), 
            p99 = quantile(usage_duration, .99), 
            p995 = quantile(usage_duration, .995), 
            p999 = quantile(usage_duration, .999)) %>% 
  pandoc.table(caption = "Usage duration outlier distribution")
```

### Average long usage duration

```{r}
thresholds <- c(.95, .99, .995, .999)

churn_to_long_usage <- NULL

for(thresh in thresholds){
  churn_to_long_usage <- rbind(churn_to_long_usage, 
                               usage %>% filter(dataset == "train") %>% 
    mutate(high_usage = usage_duration > quantile(usage_duration, thresh)) %>% 
    group_by(id) %>% 
    summarise(high_usage = mean(high_usage)) %>% 
    right_join(users %>% filter(dataset == "train") %>% select(id, churned), by = "id") %>%
    mutate(thresh = thresh))
  
}

churn_to_long_usage %>% ggplot(aes(factor(thresh), high_usage, color = churned)) + geom_boxplot()
```

Not amazing either but I'll add another factor variable: "usage higher than 95%" and impute the usage duration values above the $95^{\text{th}}$ percentile to be the $95^{\text{th}}$ percentile.

```{r impute usage duration}
usage <- usage %>% 
  mutate(long_usage_duration = usage_duration >= quantile(usage_duration, .95), 
         usage_duration = replace(usage_duration, usage_duration >= quantile(usage_duration, .95), quantile(usage_duration, .95)))

# add long usage duration
long_usage <- usage %>% 
  group_by(id) %>% 
  summarise(long_usage_duration = mean(long_usage_duration))

users <- users %>% 
  left_join(long_usage, by = "id")
```

### Average usage duration

I'll be using average usage to make these variables a bit more orthogonal to the usage count variables.

```{r average usage duration}
avg_usage_duration <- usage %>% 
  group_by(id) %>% 
  summarise(average_usage_duration = mean(usage_duration))

users <- users %>% 
  left_join(avg_usage_duration, by = "id")

users %>% filter(dataset == "train") %>% 
  ggplot(aes(churned, average_usage_duration)) + geom_boxplot()
```

Doesn't look like much of seperation. Let's try adding binning:

```{r}
users <- users %>% 
  mutate(average_usage_duration_fac = cut(average_usage_duration, quantile(average_usage_duration, seq(0, 1, .2)), include.lowest = T))

users %>% filter(dataset == "train") %>% 
  group_by(average_usage_duration_fac) %>% 
  summarise(churned = mean(churned)) %>% 
  ggplot(aes(average_usage_duration_fac, churned)) + geom_col()
```

Not amazing but I'll settle for that.

#### Average usage duration by feature

```{r}
avg_usage_duration_by_feature <- 
  usage %>% 
  group_by(id, feature_name) %>% 
  summarise(average_usage_duration = mean(usage_duration)) %>% 
  spread(key = feature_name, value = average_usage_duration, fill = 0)

names(avg_usage_duration_by_feature) <- paste0(names(avg_usage_duration_by_feature), ".average_usage_duration")

users <- users %>% 
  left_join(avg_usage_duration_by_feature, by = c("id" = "id.average_usage_duration"))

users %>% 
  filter(dataset == "train") %>% 
  select(id, Adjust.average_usage_duration:Vignette.average_usage_duration) %>% 
  gather(key = "feature_name", value = "average_usage_duration", -id) %>% 
  mutate(feature_name = gsub("\\..+", "", feature_name)) %>% 
  group_by(feature_name) %>% 
  summarise(average_usage_duration = mean(average_usage_duration)) %>% 
  mutate(feature_name = factor(feature_name, levels = feature_name[order(average_usage_duration, decreasing = T)])) %>% ggplot(aes(feature_name, average_usage_duration)) + geom_col() + theme(axis.text.x = element_text(angle = 90))
```

```{r}
users %>% filter(dataset == "train") %>% 
  select(id, churned, Adjust.average_usage_duration:Vignette.average_usage_duration) %>% 
  gather(key = "feature_name", value = "average_usage_duration", -c(id, churned)) %>% 
  ggplot(aes(feature_name, average_usage_duration, color = churned)) + 
  geom_boxplot(position = position_dodge()) + theme(axis.text.x = element_text(angle = 90))
```

Again, not amazing but I'll take it.



## Day of usage

Initially it looks reasonable that the actual day of usage during the first week wouldn't change much since the subscriber is knows to have made the decision to move to day 8. I'll thus use summary statistics without referring to the day they were made at.

## Usage count

```{r usage count, results = "asis"}
usage_count <- usage %>% 
  group_by(id) %>% 
  summarise(usage_count = n())

pandoc.table(summary(usage_count$usage_count), caption = "Usage count distribution")
```

Let's have a closer look at those outliers:

```{r usage count outliers, results="asis"}
usage_count %>% 
  summarise(p95 = quantile(usage_count, .95), 
            p99 = quantile(usage_count, .99), 
            p995 = quantile(usage_count, .995), 
            p999 = quantile(usage_count, .999)) %>% 
  pandoc.table(caption = "Usage count outlier distribution")
```
### High usage count

```{r}
thresholds <- c(.95, .99, .995, .999)

churn_to_high_usage_count <- NULL

for(thresh in thresholds){
  churn_to_high_usage_count <- rbind(churn_to_high_usage_count, 
                               usage_count %>%
    mutate(high_usage_count = usage_count > quantile(usage_count, thresh)) %>%
    right_join(users %>% filter(dataset == "train") %>%  select(id, churned), by = "id") %>% 
    group_by(high_usage_count) %>% 
    summarise(churned = mean(churned), n = n()) %>% 
      mutate(thresh = thresh))
  
}

churn_to_high_usage_count %>% ggplot(aes(factor(thresh), churned, fill = high_usage_count, label = n)) + geom_col(position = position_dodge()) + scale_y_continuous(labels = scales::percent) + geom_text(position = position_jitterdodge())
```

Not amazing either but I'll add another factor variable: "usage count greater than 95%" and impute the usage count values above the $95^{\text{th}}$ percentile to be the $95^{\text{th}}$ percentile.

```{r impute usage count}
usage_count <- usage_count %>% 
  mutate(high_usage_count = factor(usage_count > quantile(usage_count, .95)), 
         usage_count = replace(usage_count, usage_count > quantile(usage_count, .95), quantile(usage_count, .95)))

# add long usage duration
users <- users %>% 
  left_join(usage_count, by = "id")
```

### Usage count variable

```{r average usage count}
users %>% filter(dataset == "train") %>% 
  ggplot(aes(factor(churned), usage_count)) + geom_boxplot()
```

Doesn't look like much of seperation. Let's try adding binning:

```{r}
users <- users %>% 
  mutate(usage_count_fac = cut(usage_count, quantile(usage_count, seq(0, 1, .125)), include.lowest = T))

users %>% filter(dataset == "train") %>% 
  group_by(usage_count_fac) %>% 
  summarise(churned = mean(churned)) %>% 
  ggplot(aes(usage_count_fac, churned)) + geom_col() + scale_y_continuous(labels = scales::percent)
```

Not amazing but I'll settle for that.

#### Usage count by feature

```{r}
usage_count_by_feature <- 
  usage %>% 
  group_by(id, feature_name) %>% 
  summarise(usage_count = n()) %>% 
  group_by(feature_name) %>% 
  mutate(usage_count = replace(usage_count, usage_count > quantile(usage_count, .95), quantile(usage_count, .95))) %>% 
  spread(key = feature_name, value = usage_count, fill = 0)

names(usage_count_by_feature) <- paste0(names(usage_count_by_feature), ".usage_count")

users <- users %>% 
  left_join(usage_count_by_feature, by = c("id" = "id.usage_count"))

users %>% filter(dataset == "train") %>% 
  select(id, Adjust.usage_count:Vignette.usage_count) %>% 
  gather(key = "feature_name", value = "count", -id) %>% 
  mutate(feature_name = gsub("\\..+", "", feature_name)) %>% 
  group_by(feature_name) %>% 
  summarise(count = mean(count)) %>% 
  mutate(feature_name = factor(feature_name, levels = feature_name[order(count, decreasing = T)])) %>% ggplot(aes(feature_name, count)) + geom_col() + theme(axis.text.x = element_text(angle = 90))
```

```{r}
users %>% 
  filter(dataset == "train") %>% 
  select(id, churned, Adjust.usage_count:Vignette.usage_count) %>% 
  gather(key = "feature_name", value = "count", -c(id, churned)) %>% 
  mutate(feature_name = gsub("\\..+", "", feature_name)) %>% 
  ggplot(aes(feature_name, count, color = churned)) + 
  geom_boxplot(position = position_dodge()) + theme(axis.text.x = element_text(angle = 90))
```

Looks pretty bad... Hope for the best!

## feature acceptance


```{r, results = "asis"}
accepted <- usage %>% 
  group_by(id) %>% 
  summarise(accepted = mean(accepted))

pandoc.table(summary(accepted$accepted), caption = "Accepted distribution")
```


```{r}
# add long usage duration
users <- users %>% 
  left_join(accepted, by = "id")
```

### Usage count variable

```{r}
users %>% filter(dataset == "train") %>% 
  ggplot(aes(factor(churned), accepted)) + geom_boxplot()
```

Looking good. Let's just add binning for good measure

```{r}
users <- users %>% 
  mutate(accepted_fac = cut(accepted, quantile(accepted, seq(0, 1, .2)), include.lowest = T))

users %>% filter(dataset == "train") %>% 
  group_by(accepted_fac) %>% 
  summarise(churned = mean(churned)) %>% 
  ggplot(aes(accepted_fac, churned)) + geom_col() + scale_y_continuous(labels = scales::percent)
```

Very nice.

#### Feature acceptence

```{r}
usage_accepted_by_feature <- 
  usage %>% 
  group_by(id, feature_name) %>% 
  summarise(accepted = mean(accepted)) %>% 
  group_by(feature_name) %>%
  spread(key = feature_name, value = accepted, fill = 0)

names(usage_accepted_by_feature) <- paste0(names(usage_accepted_by_feature), ".accepted")

users <- users %>% 
  left_join(usage_accepted_by_feature, by = c("id" = "id.accepted"))

users %>% 
  filter(dataset == "train") %>% 
  select(id, Adjust.accepted:Vignette.accepted) %>% 
  gather(key = "feature_name", value = "accepted", -id) %>% 
  mutate(feature_name = gsub("\\..+", "", feature_name)) %>% 
  group_by(feature_name) %>% 
  summarise(accepted = mean(accepted)) %>% 
  mutate(feature_name = factor(feature_name, levels = feature_name[order(accepted, decreasing = T)])) %>% ggplot(aes(feature_name, accepted)) + geom_col() + theme(axis.text.x = element_text(angle = 90))
```

```{r}
users %>% 
  filter(dataset == "train") %>% 
  select(id, churned, Adjust.accepted:Vignette.accepted) %>% 
  gather(key = "feature_name", value = "accepted", -c(id, churned)) %>% 
  mutate(feature_name = gsub("\\..+", "", feature_name)) %>% 
  ggplot(aes(feature_name, accepted, color = factor(churned))) + 
  geom_boxplot(position = position_dodge()) + theme(axis.text.x = element_text(angle = 90))
```

OK.. that's... something...

# Model fitting

## Create task and split to train and test

```{r full features summary, results = "asis"}
names(users) <- make.names(names(users))
churn_task <- makeClassifTask(id = "churn", data = users %>% filter(dataset == "train") %>% select(-c(id, dataset)), target = "churned", positive = "1")

data.frame(variable = names(churn_task$env$data),
           class = unname(sapply(churn_task$env$data, class)),
           first_values = sapply(churn_task$env$data, function(x) paste0(head(x, 4),  collapse = ", ")),
           row.names = NULL) %>% 
  pandoc.table(caption = "Baseline features dataset")

holdout_sample <- makeResampleInstance(
  desc = makeResampleDesc(method = "Holdout", stratify = T, split = 0.8),
  task = churn_task
)

train_task <- subsetTask(churn_task, holdout_sample$train.inds[[1]])
test_task <- subsetTask(churn_task, holdout_sample$test.inds[[1]])
```

## set up learners

```{r}
# xgboost
xgboost_lrn <- makeLearner(
  cl = "classif.xgboost", 
  predict.type = "prob",
  par.vals = list(
    objective = "binary:logistic",
    eval_metric = "error",
    nrounds = 200
  )
)

xgboost_lrn <- makeDummyFeaturesWrapper(xgboost_lrn)

## tune paramters
xgboost_params <- makeParamSet(
  # The number of trees in the model (each one built sequentially)
  makeIntegerParam("nrounds", lower = 100, upper = 500),
  # number of splits in each tree
  makeIntegerParam("max_depth", lower = 1, upper = 10),
  # "shrinkage" - prevents overfitting
  makeNumericParam("eta", lower = .1, upper = .5),
  # L2 regularization - prevents overfitting
  makeNumericParam("lambda", lower = -1, upper = 0, trafo = function(x) 10^x)
)

control <- makeTuneControlRandom(maxit = 50)
resample_desc <- makeResampleDesc("CV", iters = 4)

parallelStart(mode="multicore",cpus=14,level="mlr.tuneParams")
tuned_params <- tuneParams(
  learner = xgboost_lrn,
  task = train_task,
  resampling = resample_desc,
  par.set = xgboost_params,
  control = control, 
  measures = f1
)
parallelStop()

xgboost_lrn <- setHyperPars(
  learner = xgboost_lrn,
  par.vals = tuned_params$x
)

xgboost_lrn <- makeSMOTEWrapper(xgboost_lrn, sw.rate = 1.2)

# random forest
ranger_lrn <- makeLearner(
  cl = "classif.ranger",
  predict.type = "prob"
)

ranger_params <- makeParamSet(
  # The number of variables sampled every split
  makeIntegerParam("mtry", lower = 1, upper = 30),
  # Mimimum number of observations in terminal nodes
  makeIntegerParam("min.node.size", lower = 5, upper = 50),
  # Node splitting rule
  makeDiscreteParam("splitrule", values = c("gini", "extratrees"))
)

parallelStart(mode="multicore",cpus=14,level="mlr.tuneParams")
tuned_params <- tuneParams(
  learner = ranger_lrn,
  task = train_task,
  resampling = resample_desc,
  par.set = ranger_params,
  control = control, 
  measures = f1
)
parallelStop()

ranger_lrn <- setHyperPars(
  learner = ranger_lrn,
  par.vals = tuned_params$x
)

ranger_lrn <- makeSMOTEWrapper(ranger_lrn, sw.rate = 1.2)

# cvglnet
cvglmnet_lrn <- makeLearner(
  cl = "classif.cvglmnet", 
  predict.type = "prob"
)

# logistic regression - just for reference
logistic_lrn <- makeLearner(
  cl = "classif.logreg", 
  predict.type = "prob"
)

#Let's try stacking as well:
stacked_lrn <- makeStackedLearner(base.learners = list(ranger_lrn, logistic_lrn), 
                                  super.learner = "classif.logreg", 
                                  predict.type = "prob", 
                                  method = "stack.nocv")
```

# Benchmarking

```{r bench}
bench <- benchmark(list(logistic_lrn, xgboost_lrn, ranger_lrn, cvglmnet_lrn, stacked_lrn), 
                   tasks = churn_task, 
                   resamplings = resample_desc, 
                   measures = f1)

a <- generateThreshVsPerfData(obj = bench, measures = list(tp, fn, fp, f1), gridsize = 50)
b <- a$data %>% mutate(precision = tp/(tp+fp), recall = tp/(tp + fn), f1_user1 = 2*tp/(tp*fn + fp), f1_user2 = 2*precision*recall/(precision + recall))
# check this on my own

b %>% ggplot(aes(threshold, f1, color = learner)) + geom_line()
```

# submit predictions


```{r}
optimal_thresh <- list(learner = b$learner[which.max(as.matrix(b$f1))], 
                       thresh = b$threshold[which.max(as.matrix(b$f1))], 
                       f1 = b$f1[which.max(as.matrix(b$f1))], 
                       precision = b$precision[which.max(as.matrix(b$f1))], 
                       recall = b$recall[which.max(as.matrix(b$f1))])

parallelStart(mode="multicore",cpus=14,level="mlr.tuneParams")
tuned_params <- tuneParams(
  learner = ranger_lrn,
  task = churn_task,
  resampling = resample_desc,
  par.set = ranger_params,
  control = control, 
  measures = f1
)
parallelStop()

ranger_lrn <- setHyperPars(
  learner = ranger_lrn,
  par.vals = tuned_params$x
)

final_model <- train(ranger_lrn, churn_task)

Mode <- function(x) {
  ux <- na.omit(unique(x))
  ux[which.max(tabulate(match(x, ux)))]
}

test_data <- users %>% filter(dataset == "test") %>% 
  mutate(local_subscription_hour = replace(local_subscription_hour, is.na(local_subscription_hour), Mode(local_subscription_hour)), 
         subscripiton_day_of_week = replace(subscripiton_day_of_week, is.na(subscripiton_day_of_week), Mode(subscripiton_day_of_week))
         )

predictions <- predict(final_model, newdata = test_data)
predictions <- setThreshold(predictions, optimal_thresh$thresh)

submission <- data.frame(id = test_data$id, churned = predictions$data$response)
write.csv(submission, "submission.csv", row.names = F)
```

